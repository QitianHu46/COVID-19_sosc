{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vveitch/causality-tutorials/blob/main/Sensitivity_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfZkNLUb4B-p"
      },
      "source": [
        "# Sensitivity Analysis Tutorial\n",
        "\n",
        "This tutorial gives a short example for how to assess sensitivity to unobserved confounding in causal estimation. We use the Austen plot method (https://arxiv.org/abs/2003.01747). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dS2X3Bq1-fxE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy as sp\n",
        "from sklearn import preprocessing\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingRegressor, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import KFold, StratifiedKFold, train_test_split\n",
        "from sklearn.metrics import mean_squared_error, log_loss\n",
        "import sklearn\n",
        "import os\n",
        "import pathlib\n",
        "\n",
        "from austen_plots.AustenPlot import AustenPlot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy as sp\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "from sklearn.model_selection import KFold, StratifiedKFold, train_test_split\n",
        "from sklearn.metrics import mean_squared_error, log_loss\n",
        "import sklearn\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zNsGKVyLSRxn"
      },
      "outputs": [],
      "source": [
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfZkNLUb4B-p"
      },
      "source": [
        "# ATE and ATT Estimation with Double Machine Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dS2X3Bq1-fxE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy as sp\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import KFold, StratifiedKFold, train_test_split\n",
        "from sklearn.metrics import mean_squared_error, log_loss\n",
        "import sklearn\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "nxJ46X9cFJ9X"
      },
      "outputs": [],
      "source": [
        "RANDOM_SEED=42\n",
        "np.random.seed(RANDOM_SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPbJeayiEs3u"
      },
      "source": [
        "##Load and Format Our Data (including treatment and instrument variables)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "2AC9TPko-hbt"
      },
      "outputs": [],
      "source": [
        "def make_data_covid_lockdown(df):\n",
        "    df_new = df.copy()\n",
        "    df_new['treatment'] = (df['mayor_age'] >= 52).astype(int)\n",
        "    df_new['outcome'] = (df['xc_lockdown']).astype(int)\n",
        "    return df_new\n",
        "\n",
        "col_names = ['Unnamed: 0', 'Unnamed: 0.1', 'Unnamed: 0.1.1', 'ct_shortname', 'prov',\n",
        "              'citycode', 'ctnm', 'centlon', 'centlat', 'locked_down',\n",
        "              'lockdown_date', 'bdidx_19m20', 'xc_lockdown', 'xc_closed',\n",
        "              'daySinceFirstCase', 'sub_prov_ct', 'gdp2018', '自治州-盟-地区', 'in_291',\n",
        "              'pdensity', 'gdp_p', 'hospital_d', 'popHR18_all', 'Log_popHR18_all',\n",
        "              'gdp_per_10k', 'primary_ind', 'second_ind', 'third_ind',\n",
        "              'prov_leader_rank', 'num_hospital_total', 'num_doctors_total',\n",
        "              'num_firm_total', 'non_domestic_firms_total',\n",
        "              'pct_of_non_domestic_firm', 'primary_emp_share_total',\n",
        "              'secondary_emp_share_total', 'tertiary_emp_share_total', 'inaug_time',\n",
        "              'birthmonth', 'is_female', 'age_feb20', 'party_age', 'work_age',\n",
        "              'tenure', 'majorchara', 'is_STEM_major', 'rule_in_native_prov', 'is_BA',\n",
        "              'is_MA', 'is_PhD', 'cumulative_case', 'log_cumulative_case',\n",
        "              'bdidx_19m20_feb1_10', 'lockdown_datenum', 'xc_lockdown_datenum',\n",
        "              'xc_closed_datenum', 'hu_liao_jiang_nei', 'mayor', 'mayor_age',\n",
        "              'mayor_work_age', 'xunshi', 'treatment', 'outcome']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "dataset = pd.read_csv(\"/Users/qitianhu/Documents/Research/Explore/COVID-19_sosc/STAT274/IVdata-1.csv\")\n",
        "data = make_data_covid_lockdown(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "zpdyHdlpEwQ-"
      },
      "outputs": [],
      "source": [
        "# confounder_fields = ['age_feb20']\n",
        "confounder_fields = ['sub_prov_ct', 'gdp_per_10k', 'primary_emp_share_total']\n",
        "\n",
        "confounders = data[confounder_fields]\n",
        "outcome = data['outcome']\n",
        "treatment = data['treatment']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C576dWRsa3ad"
      },
      "source": [
        "## Specify Nuisance Function Models\n",
        "\n",
        "The next step is to specify models for the conditional expected outcome and propensity score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qyOhSZRQRb8W",
        "outputId": "80b6b139-67df-4dd6-c789-f71aeef9b722"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test MSE of fit model 0.2427480768523469\n",
            "Test MSE of no-covariate model 0.2315443252770688\n"
          ]
        }
      ],
      "source": [
        "# specify a model for the conditional expected outcome\n",
        "\n",
        "# make a function that returns a sklearn model for later use in k-folding\n",
        "def make_Q_model():\n",
        "  return RandomForestRegressor(random_state=RANDOM_SEED, n_estimators=2000, max_depth=2)\n",
        "  # return LinearRegression()\n",
        "Q_model = make_Q_model()\n",
        "\n",
        "# Sanity check that chosen model actually improves test error\n",
        "# A real analysis should give substantial attention to model selection and validation \n",
        "\n",
        "X_w_treatment = confounders.copy()\n",
        "X_w_treatment[\"treatment\"] = treatment\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_w_treatment, outcome, test_size=0.2)\n",
        "Q_model.fit(X_train, y_train)\n",
        "y_pred = Q_model.predict(X_test)\n",
        "\n",
        "test_mse=mean_squared_error(y_pred, y_test)\n",
        "print(f\"Test MSE of fit model {test_mse}\") \n",
        "baseline_mse=mean_squared_error(y_train.mean()*np.ones_like(y_test), y_test)\n",
        "print(f\"Test MSE of no-covariate model {baseline_mse}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uq6eZEBXbsaI",
        "outputId": "84031124-17cc-4943-ec43-f836e87cfb66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test CE of fit model 0.7149865876129864\n",
            "Test CE of no-covariate model 0.7068502979207376\n"
          ]
        }
      ],
      "source": [
        "# specify a model for the propensity score\n",
        "\n",
        "def make_g_model():\n",
        "  # return LogisticRegression(max_iter=1000)\n",
        "  return RandomForestClassifier(n_estimators=2000, max_depth=2)\n",
        "  # return LogisticRegression()\n",
        "  # return DecisionTreeClassifier()\n",
        "\n",
        "g_model = make_g_model()\n",
        "# Sanity check that chosen model actually improves test error\n",
        "# A real analysis should give substantial attention to model selection and validation \n",
        "\n",
        "X_train, X_test, a_train, a_test = train_test_split(confounders, treatment, test_size=0.2)\n",
        "g_model.fit(X_train, a_train)\n",
        "a_pred = g_model.predict_proba(X_test)[:,1]\n",
        "\n",
        "test_ce=log_loss(a_test, a_pred)\n",
        "print(f\"Test CE of fit model {test_ce}\") \n",
        "baseline_ce=log_loss(a_test, a_train.mean()*np.ones_like(a_test))\n",
        "print(f\"Test CE of no-covariate model {baseline_ce}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RkvV_4_dFWo"
      },
      "source": [
        "## Use cross fitting to get predicted outcomes and propensity scores for each unit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "KA0AsEGJ_X3b"
      },
      "outputs": [],
      "source": [
        "# helper functions to implement the cross fitting\n",
        "\n",
        "def treatment_k_fold_fit_and_predict(make_model, X:pd.DataFrame, A:np.array, n_splits:int):\n",
        "    \"\"\"\n",
        "    Implements K fold cross-fitting for the model predicting the treatment A. \n",
        "    That is, \n",
        "    1. Split data into K folds\n",
        "    2. For each fold j, the model is fit on the other K-1 folds\n",
        "    3. The fitted model is used to make predictions for each data point in fold j\n",
        "    Returns an array containing the predictions  \n",
        "\n",
        "    Args:\n",
        "    model: function that returns sklearn model (which implements fit and predict_prob)\n",
        "    X: dataframe of variables to adjust for\n",
        "    A: array of treatments\n",
        "    n_splits: number of splits to use\n",
        "    \"\"\"\n",
        "    predictions = np.full_like(A, np.nan, dtype=float)\n",
        "    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_SEED)\n",
        "    \n",
        "    for train_index, test_index in kf.split(X, A):\n",
        "      X_train = X.loc[train_index]\n",
        "      A_train = A.loc[train_index]\n",
        "      g = make_model()\n",
        "      g.fit(X_train, A_train)\n",
        "\n",
        "      # get predictions for split\n",
        "      predictions[test_index] = g.predict_proba(X.loc[test_index])[:, 1]\n",
        "\n",
        "    assert np.isnan(predictions).sum() == 0\n",
        "    return predictions\n",
        "\n",
        "\n",
        "def outcome_k_fold_fit_and_predict(make_model, X:pd.DataFrame, y:np.array, A:np.array, n_splits:int, output_type:str):\n",
        "    \"\"\"\n",
        "    Implements K fold cross-fitting for the model predicting the outcome Y. \n",
        "    That is, \n",
        "    1. Split data into K folds\n",
        "    2. For each fold j, the model is fit on the other K-1 folds\n",
        "    3. The fitted model is used to make predictions for each data point in fold j\n",
        "    Returns two arrays containing the predictions for all units untreated, all units treated  \n",
        "\n",
        "    Args:\n",
        "    model: function that returns sklearn model (that implements fit and either predict_prob or predict)\n",
        "    X: dataframe of variables to adjust for\n",
        "    y: array of outcomes\n",
        "    A: array of treatments\n",
        "    n_splits: number of splits to use\n",
        "    output_type: type of outcome, \"binary\" or \"continuous\"\n",
        "\n",
        "    \"\"\"\n",
        "    predictions0 = np.full_like(A, np.nan, dtype=float)\n",
        "    predictions1 = np.full_like(y, np.nan, dtype=float)\n",
        "    if output_type == 'binary':\n",
        "      kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_SEED)\n",
        "    elif output_type == 'continuous':\n",
        "      kf = KFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_SEED)\n",
        "\n",
        "    # include the treatment as input feature\n",
        "    X_w_treatment = X.copy()\n",
        "    X_w_treatment[\"A\"] = A\n",
        "\n",
        "    # for predicting effect under treatment / control status for each data point \n",
        "    X0 = X_w_treatment.copy()\n",
        "    X0[\"A\"] = 0\n",
        "    X1 = X_w_treatment.copy()\n",
        "    X1[\"A\"] = 1\n",
        "\n",
        "    \n",
        "    for train_index, test_index in kf.split(X_w_treatment, y):\n",
        "      X_train = X_w_treatment.loc[train_index]\n",
        "      y_train = y.loc[train_index]\n",
        "      q = make_model()\n",
        "      q.fit(X_train, y_train)\n",
        "\n",
        "      if output_type =='binary':\n",
        "        predictions0[test_index] = q.predict_proba(X0.loc[test_index])[:, 1]\n",
        "        predictions1[test_index] = q.predict_proba(X1.loc[test_index])[:, 1]\n",
        "      elif output_type == 'continuous':\n",
        "        predictions0[test_index] = q.predict(X0.loc[test_index])\n",
        "        predictions1[test_index] = q.predict(X1.loc[test_index])\n",
        "\n",
        "    assert np.isnan(predictions0).sum() == 0\n",
        "    assert np.isnan(predictions1).sum() == 0\n",
        "    return predictions0, predictions1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "wVcE6pRQeMNf"
      },
      "outputs": [],
      "source": [
        "g = treatment_k_fold_fit_and_predict(make_g_model, X=confounders, A=treatment, n_splits=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "GLEHlLLdWSh9"
      },
      "outputs": [],
      "source": [
        "Q0,Q1=outcome_k_fold_fit_and_predict(make_Q_model, X=confounders, y=outcome, A=treatment, n_splits=10, output_type=\"continuous\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "_NVCV0q0g8wQ",
        "outputId": "cb2569e4-65eb-4c82-df21-790a0a4568c9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>g</th>\n",
              "      <th>Q0</th>\n",
              "      <th>Q1</th>\n",
              "      <th>A</th>\n",
              "      <th>Y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.576787</td>\n",
              "      <td>0.374757</td>\n",
              "      <td>0.401724</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.561690</td>\n",
              "      <td>0.260766</td>\n",
              "      <td>0.292559</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.543547</td>\n",
              "      <td>0.351407</td>\n",
              "      <td>0.380547</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.501709</td>\n",
              "      <td>0.425407</td>\n",
              "      <td>0.441105</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.551096</td>\n",
              "      <td>0.338761</td>\n",
              "      <td>0.359490</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          g        Q0        Q1  A  Y\n",
              "0  0.576787  0.374757  0.401724  1  0\n",
              "1  0.561690  0.260766  0.292559  0  0\n",
              "2  0.543547  0.351407  0.380547  0  0\n",
              "3  0.501709  0.425407  0.441105  1  0\n",
              "4  0.551096  0.338761  0.359490  0  1"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_and_nuisance_estimates = pd.DataFrame({'g': g, 'Q0': Q0, 'Q1': Q1, 'A': treatment, 'Y': outcome})\n",
        "data_and_nuisance_estimates.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNhM7URdgzQB"
      },
      "source": [
        "## Combine predicted values and data into estimate of ATT\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "J-vONC5ejwh2"
      },
      "outputs": [],
      "source": [
        "def att_aiptw(Q0, Q1, g, A, Y, prob_t=None):\n",
        "  \"\"\"\n",
        "  # Double ML estimator for the ATT\n",
        "  This uses the ATT specific scores, see equation 3.9 of https://www.econstor.eu/bitstream/10419/149795/1/869216953.pdf\n",
        "  \"\"\"\n",
        "\n",
        "  if prob_t is None:\n",
        "    prob_t = A.mean() # estimate marginal probability of treatment\n",
        "\n",
        "  tau_hat = (A*(Y-Q0) - (1-A)*(g/(1-g))*(Y-Q0)).mean()/ prob_t\n",
        "  \n",
        "  scores = (A*(Y-Q0) - (1-A)*(g/(1-g))*(Y-Q0) - tau_hat*A) / prob_t\n",
        "  n = Y.shape[0] # number of observations\n",
        "  std_hat = np.std(scores) / np.sqrt(n)\n",
        "\n",
        "  return tau_hat, std_hat\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "O_F5r0SSkzzK"
      },
      "outputs": [],
      "source": [
        "def ate_aiptw(Q0, Q1, g, A, Y, prob_t=None):\n",
        "  \"\"\"\n",
        "  # Double ML estimator for the ATE\n",
        "  \"\"\"\n",
        "\n",
        "  tau_hat = (Q1 - Q0 + A*(Y-Q1)/g - (1-A)*(Y-Q0)/(1-g)).mean()\n",
        "  \n",
        "  scores = Q1 - Q0 + A*(Y-Q1)/g - (1-A)*(Y-Q0)/(1-g) - tau_hat\n",
        "  n = Y.shape[0] # number of observations\n",
        "  std_hat = np.std(scores) / np.sqrt(n)\n",
        "\n",
        "  return tau_hat, std_hat\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SjDj0F9Bm9uq",
        "outputId": "d99df54c-d3a5-4eb9-e041-7a6b50317430"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The estimate is 0.09732946126478602 pm 0.11957760109630337\n"
          ]
        }
      ],
      "source": [
        "# new\n",
        "tau_hat, std_hat = att_aiptw(**data_and_nuisance_estimates)\n",
        "print(f\"The estimate is {tau_hat} pm {1.96*std_hat}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SjDj0F9Bm9uq",
        "outputId": "d99df54c-d3a5-4eb9-e041-7a6b50317430"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The estimate is 0.010259044357832365 pm 0.13533333724187355\n"
          ]
        }
      ],
      "source": [
        "tau_hat, std_hat = att_aiptw(**data_and_nuisance_estimates)\n",
        "print(f\"The estimate is {tau_hat} pm {1.96*std_hat}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The estimate is 0.0829477203423855 pm 0.14919629150425842\n"
          ]
        }
      ],
      "source": [
        "# new\n",
        "in_treated = data_and_nuisance_estimates['A']==1\n",
        "treated_estimates = data_and_nuisance_estimates[in_treated]\n",
        "tau_hat, std_hat = ate_aiptw(**treated_estimates)\n",
        "print(f\"The estimate is {tau_hat} pm {1.96*std_hat}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSaOp1HwlQ4i",
        "outputId": "aedac68a-beab-43b3-bc64-caf16ff7bbdf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The estimate is -0.01392439425176383 pm 0.329385027215484\n"
          ]
        }
      ],
      "source": [
        "in_treated = data_and_nuisance_estimates['A']==1\n",
        "treated_estimates = data_and_nuisance_estimates[in_treated]\n",
        "tau_hat, std_hat = ate_aiptw(**treated_estimates)\n",
        "print(f\"The estimate is {tau_hat} pm {1.96*std_hat}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOuJnlbEo8j_",
        "outputId": "d713a2da-5711-414c-9b8f-b3e8d5cad156"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The estimate is 0.010259044357832365 pm 0.13533333724187355\n"
          ]
        }
      ],
      "source": [
        "# Computing the estimate restricted to a population with only reasonable propensity scores\n",
        "g = data_and_nuisance_estimates['g']\n",
        "in_overlap_popluation = ((g > 0.05) & (g < 0.95))\n",
        "overlap_data_and_nuisance = data_and_nuisance_estimates[in_overlap_popluation]\n",
        "tau_hat, std_hat = att_aiptw(**overlap_data_and_nuisance)\n",
        "print(f\"The estimate is {tau_hat} pm {1.96*std_hat}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NkEQ6xzRjz-I"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "Sensitivity_Analysis.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
